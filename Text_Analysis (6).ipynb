{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "\n",
        "Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens will help you to understand the context or develop the model for natural language processing. The tokenization will also help to intepret the meaning of the text by analyzing the sequence of the words.\n",
        "\n",
        "https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4\n",
        "\n",
        "\n",
        "The process of tokenization starts with breaking apart words using the white space between them. But it is hard to correctly identify each token because it requires the library package to account for exceptions such as hyphens, apostrophes and a language dictionary to ensure the value is properly identified. Therefore, tokenization requires the language of origin of the text to be known to process it.\n",
        "\n",
        "\n",
        "There are several techniques used by Natural Language Processing (NLP) to solve this problem. The first solution is n-grams. This is the process of combining words within the same sentence as a group, typically 2 or 3 words, to create a pattern that is recognizable by the NLP library. \n",
        "\n",
        "Another common reference for an n-gram is a bi-gram and the bi-gram only uses 2 words. The n denotes the number of grams. Therefore, we have a unigram stands for 1 gram and a tri-gram is 3 and so on.\n",
        "\n",
        "Another solution is a bag of words, which is when a high occurenece of specific words exist in the source data. Bag of words is very helpful to identify patterns and key term searches against large text sources data."
      ],
      "metadata": {
        "id": "q1jt605YVnFL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rfhNH_tVbb8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}